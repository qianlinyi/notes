# Neural Networks and Deep Learning

## Introduction to Deep Learning

### What is a Neural Network

Housing Price Prediction

![在这里插入图片描述](https://img-blog.csdnimg.cn/3a270dbb64d84da8bcf14e42386ba016.png)

ReLU 激活函数 max(0,x)

修正线性单元 Rectified Linear Unit 取不小于 0 的值

![在这里插入图片描述](https://img-blog.csdnimg.cn/0ac5757545024bb3b79a651c234c3e2c.png)

神经元可以看作是乐高积木，可以通过搭积木的方式获得一个更大的神经网络

![在这里插入图片描述](https://img-blog.csdnimg.cn/d0526b6c7af5431894be77d19e513a76.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/645c52a1b1364fbf9aa221570dfb923b.png)

神经网络的一部分神奇之处在于，当你实现它之后，你要做的只是输入 x，就能得到输出 y。因为它可以自己计算你训练集中样本的数目以及所有的中间过程。所以，你实际上要做的就是：这里有四个输入的神经网络，这输入的特征可能是房屋的大小、卧室的数量、邮政编码和区域的富裕程度。给出这些输入的特征之后，神经网络的工作就是预测对应的价格。同时也注意到这些被叫做隐藏单元圆圈，在一个神经网络中，它们每个都从输入的四个特征获得自身输入，比如说，第一个结点代表家庭人口，而家庭人口仅仅取决于 $x_1$  和 $x_2$  特征，换句话说，在神经网络中，你决定在这个结点中想要得到什么，然后用所有的四个输入来计算想要得到的。因此，我们说输入层和中间层被紧密的连接起来了。

### Supervised Learning with Neural Networks

关于神经网络也有很多的种类，考虑到它们的使用效果，有些使用起来恰到好处，但事实表明，到目前几乎所有由神经网络创造的经济价值，本质上都离不开一种叫做监督学习的机器学习类别，让我们举例看看。

在监督学习中你有一些输入 x ，你想学习到一个函数来映射到一些输出 y，比如我们之前提到的房价预测的例子，你只要输入有关房屋的一些特征，试着去输出或者估计价格 y。我们举一些其它的例子，来说明神经网络已经被高效应用到其它地方。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200403222046889.png)

对于房地产和在线广告老师，经常使用相对标准一些的神经网络

对于图像数据，经常使用卷积神经网络 CNN

对于序列数据，例如音频，经常使用循环神经网络 RNN

![在这里插入图片描述](https://img-blog.csdnimg.cn/28315a3f673f4d9e902898c3e8c9573a.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/68f3f2b0ce3440f997fface12e87c095.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200403222312201.png)

### Why is Deep Learning taking off ？

在水平轴上画一个形状，在此绘制出所有任务的数据量，而在垂直轴上，画出机器学习算法的性能。比如说准确率体现在垃圾邮件过滤或者广告点击预测，或者是神经网络在自动驾驶汽车时判断位置的准确性，根据图像可以发现，如果你把一个传统机器学习算法的性能画出来，作为数据量的一个函数，你可能得到一个弯曲的线，就像图中这样，它的性能一开始在增加更多数据时会上升，但是一段变化后它的性能就会像一个平原一样。假设你的水平轴拉的很长很长，它们不知道如何处理规模巨大的数据，而过去十年的社会里，我们遇到的很多问题只有相对较少的数据量。

![在这里插入图片描述](https://img-blog.csdnimg.cn/202004032224337.png)

多亏数字化社会的来临，现在的数据量都非常巨大，我们花了很多时间活动在这些数字的领域，比如在电脑网站上、在手机软件上以及其它数字化的服务，它们都能创建数据，同时便宜的相机被配置到移动电话，还有加速仪及各类各样的传感器，同时在物联网领域我们也收集到了越来越多的数据。仅仅在过去的 20 年里对于很多应用，我们便收集到了大量的数据，远超过机器学习算法能够高效发挥它们优势的规模。

神经网络展现出的是，如果你训练一个小型的神经网络，那么这个性能可能会像下图黄色曲线表示那样；如果你训练一个稍微大一点的神经网络，比如说一个中等规模的神经网络（下图蓝色曲线），它在某些数据上面的性能也会更好一些；如果你训练一个非常大的神经网络，它就会变成下图绿色曲线那样，并且保持变得越来越好。因此可以注意到两点：如果你想要获得较高的性能体现，那么你有两个条件要完成，第一个是你需要训练一个规模足够大的神经网络，以发挥数据规模量巨大的优点，另外你需要能画到 x xx 轴的这个位置，所以你需要很多的数据。因此我们经常说规模一直在推动深度学习的进步，这里的规模指的也同时是神经网络的规模，我们需要一个带有许多隐藏单元的神经网络，也有许多的参数及关联性，就如同需要大规模的数据一样。事实上如今最可靠的方法来在神经网络上获得更好的性能，往往就是要么训练一个更大的神经网络，要么投入更多的数据，这只能在一定程度上起作用，因为最终你耗尽了数据，或者最终你的网络是如此大规模导致将要用太久的时间去训练，但是仅仅提升规模的的确确地让我们在深度学习的世界中摸索了很多时间。为了使这个图更加从技术上讲更精确一点，我在 x 轴下面已经写明的数据量，这儿加上一个标签（label）量，通过添加这个标签量，也就是指在训练样本时，我们同时输入 x 和标签 y，接下来引入一点符号，使用小写的字母 m 表示训练集的规模，或者说训练样本的数量，这个小写字母 m 就横轴结合其他一些细节到这个图像中。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200403222612316.png)

所以可以这么说，在深度学习萌芽的初期，数据的规模以及计算量，局限在我们对于训练一个特别大的神经网络的能力，无论是在CPU还是GPU上面，那都使得我们取得了巨大的进步。但是渐渐地，尤其是在最近这几年，我们也见证了算法方面的极大创新。许多算法方面的创新，一直是在尝试着使得神经网络运行的更快。

作为一个具体的例子，神经网络方面的一个巨大突破是从 sigmoid 函数转换到一个 ReLU 函数，这个函数我们在之前的课程里提到过。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200403222654711.png)

如果你无法理解刚才我说的某个细节，也不需要担心，可以知道的一个使用sigmoid函数和机器学习问题是，在这个区域，也就是这个sigmoid函数的梯度会接近零，所以学习的速度会变得非常缓慢，因为当你实现梯度下降以及梯度接近零的时候，参数会更新的很慢，所以学习的速率也会变的很慢，而通过改变这个被叫做激活函数的东西，神经网络换用这一个函数，叫做ReLU的函数（修正线性单元），ReLU它的梯度对于所有输入的负值都是零，因此梯度更加不会趋向逐渐减少到零。而这里的梯度，这条线的斜率在这左边是零，仅仅通过将Sigmod函数转换成ReLU函数，便能够使得一个叫做梯度下降（gradient descent）的算法运行的更快，这就是一个或许相对比较简单的算法创新的例子。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200403222744990.png)
