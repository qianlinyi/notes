# Course 1 Neural Networks and Deep Learning

## Week 1 Introduction to Deep Learning

### 1.1 What is a Neural Network

Housing Price Prediction

![在这里插入图片描述](https://img-blog.csdnimg.cn/3a270dbb64d84da8bcf14e42386ba016.png)

ReLU 激活函数 max(0,x)

修正线性单元 Rectified Linear Unit 取不小于 0 的值

![在这里插入图片描述](https://img-blog.csdnimg.cn/0ac5757545024bb3b79a651c234c3e2c.png)

神经元可以看作是乐高积木，可以通过搭积木的方式获得一个更大的神经网络

![在这里插入图片描述](https://img-blog.csdnimg.cn/d0526b6c7af5431894be77d19e513a76.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/645c52a1b1364fbf9aa221570dfb923b.png)

神经网络的一部分神奇之处在于，当你实现它之后，你要做的只是输入 x，就能得到输出 y。因为它可以自己计算你训练集中样本的数目以及所有的中间过程。所以，你实际上要做的就是：这里有四个输入的神经网络，这输入的特征可能是房屋的大小、卧室的数量、邮政编码和区域的富裕程度。给出这些输入的特征之后，神经网络的工作就是预测对应的价格。同时也注意到这些被叫做隐藏单元圆圈，在一个神经网络中，它们每个都从输入的四个特征获得自身输入，比如说，第一个结点代表家庭人口，而家庭人口仅仅取决于 $x_1$  和 $x_2$  特征，换句话说，在神经网络中，你决定在这个结点中想要得到什么，然后用所有的四个输入来计算想要得到的。因此，我们说输入层和中间层被紧密的连接起来了。

### 1.2 Supervised Learning with Neural Networks

关于神经网络也有很多的种类，考虑到它们的使用效果，有些使用起来恰到好处，但事实表明，到目前几乎所有由神经网络创造的经济价值，本质上都离不开一种叫做监督学习的机器学习类别，让我们举例看看。

在监督学习中你有一些输入 x ，你想学习到一个函数来映射到一些输出 y，比如我们之前提到的房价预测的例子，你只要输入有关房屋的一些特征，试着去输出或者估计价格 y。我们举一些其它的例子，来说明神经网络已经被高效应用到其它地方。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200403222046889.png)

对于房地产和在线广告老师，经常使用相对标准一些的神经网络

对于图像数据，经常使用卷积神经网络 CNN

对于序列数据，例如音频，经常使用循环神经网络 RNN

![在这里插入图片描述](https://img-blog.csdnimg.cn/28315a3f673f4d9e902898c3e8c9573a.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/68f3f2b0ce3440f997fface12e87c095.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200403222312201.png)

### 1.3 Why is Deep Learning taking off ？

在水平轴上画一个形状，在此绘制出所有任务的数据量，而在垂直轴上，画出机器学习算法的性能。比如说准确率体现在垃圾邮件过滤或者广告点击预测，或者是神经网络在自动驾驶汽车时判断位置的准确性，根据图像可以发现，如果你把一个传统机器学习算法的性能画出来，作为数据量的一个函数，你可能得到一个弯曲的线，就像图中这样，它的性能一开始在增加更多数据时会上升，但是一段变化后它的性能就会像一个平原一样。假设你的水平轴拉的很长很长，它们不知道如何处理规模巨大的数据，而过去十年的社会里，我们遇到的很多问题只有相对较少的数据量。

![在这里插入图片描述](https://img-blog.csdnimg.cn/202004032224337.png)

多亏数字化社会的来临，现在的数据量都非常巨大，我们花了很多时间活动在这些数字的领域，比如在电脑网站上、在手机软件上以及其它数字化的服务，它们都能创建数据，同时便宜的相机被配置到移动电话，还有加速仪及各类各样的传感器，同时在物联网领域我们也收集到了越来越多的数据。仅仅在过去的 20 年里对于很多应用，我们便收集到了大量的数据，远超过机器学习算法能够高效发挥它们优势的规模。

神经网络展现出的是，如果你训练一个小型的神经网络，那么这个性能可能会像下图黄色曲线表示那样；如果你训练一个稍微大一点的神经网络，比如说一个中等规模的神经网络（下图蓝色曲线），它在某些数据上面的性能也会更好一些；如果你训练一个非常大的神经网络，它就会变成下图绿色曲线那样，并且保持变得越来越好。因此可以注意到两点：如果你想要获得较高的性能体现，那么你有两个条件要完成，第一个是你需要训练一个规模足够大的神经网络，以发挥数据规模量巨大的优点，另外你需要能画到 x xx 轴的这个位置，所以你需要很多的数据。因此我们经常说规模一直在推动深度学习的进步，这里的规模指的也同时是神经网络的规模，我们需要一个带有许多隐藏单元的神经网络，也有许多的参数及关联性，就如同需要大规模的数据一样。事实上如今最可靠的方法来在神经网络上获得更好的性能，往往就是要么训练一个更大的神经网络，要么投入更多的数据，这只能在一定程度上起作用，因为最终你耗尽了数据，或者最终你的网络是如此大规模导致将要用太久的时间去训练，但是仅仅提升规模的的确确地让我们在深度学习的世界中摸索了很多时间。为了使这个图更加从技术上讲更精确一点，我在 x 轴下面已经写明的数据量，这儿加上一个标签（label）量，通过添加这个标签量，也就是指在训练样本时，我们同时输入 x 和标签 y，接下来引入一点符号，使用小写的字母 m 表示训练集的规模，或者说训练样本的数量，这个小写字母 m 就横轴结合其他一些细节到这个图像中。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200403222612316.png)

所以可以这么说，在深度学习萌芽的初期，数据的规模以及计算量，局限在我们对于训练一个特别大的神经网络的能力，无论是在CPU还是GPU上面，那都使得我们取得了巨大的进步。但是渐渐地，尤其是在最近这几年，我们也见证了算法方面的极大创新。许多算法方面的创新，一直是在尝试着使得神经网络运行的更快。

作为一个具体的例子，神经网络方面的一个巨大突破是从 sigmoid 函数转换到一个 ReLU 函数，这个函数我们在之前的课程里提到过。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200403222654711.png)

如果你无法理解刚才我说的某个细节，也不需要担心，可以知道的一个使用sigmoid函数和机器学习问题是，在这个区域，也就是这个sigmoid函数的梯度会接近零，所以学习的速度会变得非常缓慢，因为当你实现梯度下降以及梯度接近零的时候，参数会更新的很慢，所以学习的速率也会变的很慢，而通过改变这个被叫做激活函数的东西，神经网络换用这一个函数，叫做ReLU的函数（修正线性单元），ReLU它的梯度对于所有输入的负值都是零，因此梯度更加不会趋向逐渐减少到零。而这里的梯度，这条线的斜率在这左边是零，仅仅通过将Sigmod函数转换成ReLU函数，便能够使得一个叫做梯度下降（gradient descent）的算法运行的更快，这就是一个或许相对比较简单的算法创新的例子。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200403222744990.png)

## Week 2 Basics of Neural Network Programming

### 2.1 Binary Classification

logistic regression 逻辑回归

逻辑回归是一个用于二分类(binary classification)的算法。首先我们从一个问题开始说起，这里有一个二分类问题的例子，假如你有一张图片作为输入，比如这只猫，如果识别这张图片为猫，则输出标签1作为结果；如果识别出不是猫，那么输出标签0作为结果。现在我们可以用字母 来 表示输出的结果标签，如下图所示：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404155904324.png)

我们来看看一张图片在计算机中是如何表示的，为了保存一张图片，需要保存三个矩阵，它们分别对应图片中的红、绿、蓝三种颜色通道，如果你的图片大小为 64x64 像素，那么你就有三个规模为 64x64 的矩阵，分别对应图片中红、绿、蓝三种像素的强度值。为了便于表示，这里我画了三个很小的矩阵，注意它们的规模为 5x4 而不是64x64，如下图所示：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404155924145.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404160212670.png)

符号定义

$x$: 表示一个 $n_x$ 维数据，为输入数据，维度为 $(n_x,1)$；

$y$: 表示输出结果，取值为 $(0,1)$；

$(x^{(i)},y^{(i)})$: 表示第 $i$ 组数据，可能是训练数据，也可能是测试数据，此处默认为训练数据；

$X=[x^{(1)},x^{(2)},...,x^{(m)}]$: 表示所有的训练数据集的输入值，放入一个 $n_x\times m$ 的矩阵中，其中 $m$ 代表样本数量；

$Y=[y^{(1)},y^{(2)},...,y^{(m)}]$: 表示所有训练数据集的输出值，维度为 $1\times m$。

用一对 $(x,y)$ 来表示一个单独的样本，$x$ 表示 $n_x$ 维的特征向量，$y$ 表示标签（输出结果）只能为 0 或 1。而训练集将由 $m$ 个训练样本组成，其中 $(x^{(1)},y^{(1)})$ 表示第一个样本的输入和输出，直到最后一个样本 $(x^{(m)},y^{(m)})$ ，然后所有的这些一起表示整个训练集。有时候为了强调这是训练样本的个数，会写作 $M_{train}$，当涉及到测试集的时候，我们会使用 $M_{test}$ 来表示测试集的样本数，所以这是测试集的样本数：

![image-20220914164213984](/Users/zaizai/Library/Application Support/typora-user-images/image-20220914164213984.png)

最后为了能把训练集表示得更紧凑一点，我们会定义一个矩阵 $X$，它由输入向量 $x^{(1)},x^{(2)}$ 等组成，如下图放在矩阵的列中，所以我们把 $x^{(1)}$ 放到矩阵的第一列中，$x^{(2)}$ 放到第二列，$x^{(m)}$ 放到第 $m$ 列，$m$ 是训练集的样本数量，然后这个矩阵的高度记为 $n_x$，注意有时候可能因为其他某些原因，矩阵 $X$ 会由训练样本按照行堆叠起来而不是列，如下图所示：$x^{(1)}$ 到 $x^{(m)}$ 的转置，但在实现神经网络的时候，使用左边的形式，会让实现过程更加简单。

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020040416120555.png)

$X$ 是一个规模为 $n_x\times m$ 的矩阵，当你用 $Python$ 实现的时候，你会看到 $X.shape$，用以显示矩阵的规模，即 $X.shape$ 等于 $(n_x,m)$

同理，可以把标签 $y$ 放在列中将会简化后续计算，可以定义大些的 $Y$ 等于 $y^{(1)},y^{(2)},...,y^{(m)}$，所以这里是一个规模为 $1\times m$ 的矩阵，$Y.shape = (1,m)$

一个好的符号约定能够将不同训练样本的数据很好地组织起来

### 2.2 Logistic Regression

对于二分类问题，给定输入 $X$，则有输出预测 $\hat{y}=P(y=1|x)$。上文提到过，$X$ 是一个 $n_x$ 维的向量，用 $w$ 表示逻辑回归的参数，$b$ 表示偏差，则有 $\hat{y}=w^Tx+b$

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404162203802.png)

此时得到一个关于输入 $x$ 的线性函数，实际上这是在做线性回归时用到的，但是对于二分类问题来讲并不是一个好的算法，所以可以将上式作为自变量放到 sigmoid 函数中，可以将线性函数转化为非线性函数。

下图为 sigmoid 函数的图像，$\sigma(z)=\frac{1}{1+e^{-z}}$：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404162822815.png)

逻辑回归就是调整参数 $w$ 和 $b$，让 $\hat{y}$ 变得更加准确。在神经网络的学习中，通常将参数 $w$ 和 $b$ 分开，在这里参数 $b$ 是一种偏置。还有一种处理此问题的其他符号表示，如下图所示：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404163746291.png)

### 2.3 Logistic Regression cost function

为了训练逻辑回归模型的参数 w 和参数 b ，我们需要一个代价函数，通过训练代价函数来得到参数 w 和参数 b。先看一下逻辑回归的输出函数：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404164350355.png)

损失函数（误差函数）$L(\hat{y},y)$

用来衡量算法的运行情况，即预测值和真实值的接近程度，通常为预测值和真实值差的平方或平方的一半，但是在逻辑回归中一般不这么做，因为在学习逻辑回归参数时，会发现优化目标不是凸优化，只能得到多个局部最优值，用梯度下降法可能找不到全局最优值，所以通常会定义另一个损失函数：

$L(\hat{y},y)=−y\log(\hat{y})−(1−y)\log(1−\hat{y})$

当 $y=1$ 时，$L=-log(\hat{y})$，如果想要损失函数 $L$ 尽可能小，那么 $\hat{y}$ 就要尽可能大，因为 sigmoid 函数的缘故，$\hat{y}$ 会无限接近于 1

当 $y=0$ 时，$L=-\log(1-\hat{y})$，如果想要损失函数 $L$ 尽可能小，那么 $\hat{y}$ 就要尽可能小，因为 sigmoid 函数的缘故，$\hat{y}$ 会无限接近于 0

其实有很多函数和这个函数效果类似，就是如果 y 等于 1，就尽可能让 $\hat{y}$ 变大；如果 y 等于 0，就尽可能让 $\hat{y}$ 变小。**损失函数是在单个训练样本中定义的，它衡量的是算法在单个训练样本中的表现。**

为了衡量算法在全部训练样本上的表现，需要定义一个算法的代价函数，对 m 个样本的损失求和然后除以 m：

$J(w,b)=\frac{1}{m}\sum\limits_{i=1}^m{L(\hat{y}^{(i)},y^{(i)})=\frac{1}{m}\sum\limits_{i=1}^m{(−y^{(i)}\log\hat{y}^{(i)}−(1−y^{(i)})\log(1−\hat{y}^{(i)}))}}$

损失函数只适用于单个训练样本，而代价函数是参数的总代价，所以在训练逻辑回归模型时候，我们需要找到合适的 w 和 b ，来让代价函数 J 的总代价降到最低。 

### 2.4 Gradient Descent

在测试集上，通过最小化代价函数（成本函数）$J(w,b)$ 来训练参数 w 和 b，如图所示，在第二行给出和之前一样的逻辑回归算法的代价函数

![在这里插入图片描述](https://img-blog.csdnimg.cn/202004041658320.png)

**梯度下降法的形象化说明：**

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404165858277.png#pic_center)

在这个图中，横轴表示你的空间参数 w 和 b ，在实践中， w 可以是更高的维度，但是为了更好地绘图，我们定义 w 和 b 都是单一实数，代价函数（成本函数）$ J(w,b)$ 是在水平轴 w 和 b 上的曲面，因此曲面的高度就是 $J(w,b)$ 在某一点的函数值。我们所做的就是找到使得代价函数（成本函数） $J(w,b)$ 函数值是最小值，对应的参数 w 和 b。

<img src="https://img-blog.csdnimg.cn/2020040417005568.png" alt="在这里插入图片描述" style="text-align:right" />

如上图所示，代价函数是一个凸函数，像一个大碗一样。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404170131979.png)

如上图所示，这就与刚才的图有些相反，因为它是非凸的并且有很多局部最优解。由于逻辑回归的代价函数 $J(w,b)$ 特性，必须定义其为凸函数，初始化 w 和 b，可以用如图那个小红点来初始化参数 w 和 b，也可以采用随机初始化的方法，对于逻辑回归几乎所有的初始化方法都有效，因为无论在哪里初始化，都可以达到同一点或大致相同的点。

假定代价函数 $J(w)$ 仅有一个参数 w，即用一维曲线代替多维曲线，这样可以更好地画出图像。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404170454962.png#pic_center)

迭代就是不断重复如图的公式：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404170526302.png#pic_center)

$\alpha$ 表示学习率，用来控制步长，即向下走一步的长度，$\frac{dJ(w)}{dw}$ 就是函数 $J(w)$ 对 w 求导，在代码中我们会用 $dw$ 表示这个结果

逻辑回归的代价函数 $J(w,b)$ 是含有两个参数的。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404170906568.png#pic_center)

$\sigma$ 表示求偏导符号，当函数含有两个以上的参数时会用到

### 2.5 Computation Graph

可以说，一个神经网络的计算，都是按照前向或反向传播过程组织的。首先我们计算出一个新的网络的输出（前向过程），紧接着进行一个反向传输操作。后者我们用来计算出对应的梯度或导数。计算图解释了为什么我们用这种方式组织这些计算过程。在这个视频中，我们将举一个例子说明计算图是什么。让我们举一个比逻辑回归更加简单的，或者说不那么正式的神经网络的例子。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404192554622.png)

### 2.6 Derivatives with a Computation Graph 

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404181258919.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404181411904.png)

### 2.7 Logistic Regression Gradient descent 

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404215803496.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404221156854.png)

### 2.8 Gradient descent on m examples

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020040422245014.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404222948517.png)

代码如下：

```python
J = dw1 = dw2 = db = 0
for i in range(1, m + 1):
    z(i) = w * x(i) + b
    a(i) = sigmoid(z(i))
    J += -[y(i) * log(a(i)) + (1 - y(i)) * log(1 - a(i))]
    dz(i) = a(i) - y(i)
    dw1 += x1(i) * dz(i)
    dw2 += x2(i) * dz(i)
    db += dz(i)
J /= m
dw1 /= m
dw2 /= m
db /= m
w = w - alpha * dw
b = b - alpha * db
```

幻灯片上只应用了一步梯度下降。因此需要重复以上内容很多次，以应用多次梯度下降。

当你应用深度学习算法，你会发现在代码中显式地使用for循环使你的算法很低效，同时在深度学习领域会有越来越大的数据集。所以能够应用你的算法且没有显式的for循环会是重要的，并且会帮助你适用于更大的数据集。所以这里有一些叫做向量化技术,它可以允许你的代码摆脱这些显式的for循环。

### 2.9 Vectorization

向量化是非常基础的去除代码中**for**循环的艺术，在深度学习安全领域、深度学习实践中，你会经常发现自己需要训练大数据集，因为深度学习算法处理大数据集效果很棒，所以你的代码运行速度非常重要，否则你的代码可能花费很长时间去运行，且要等待非常长的时间得到结果。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200405174859641.png)

向量化和 for 循环运行速度比较：[代码地址](https://github.com/qianlinyi/deep-learning/blob/main/vectorization%20demo.ipynb)

你可能听过很多类似如下的话，“大规模的深度学习使用了 GPU 或者图像处理单元实现”，但是我做的所有的案例都是在jupyter notebook上面实现，这里只有 CPU，CPU 和 GPU 都有并行化的指令，他们有时候会叫做 SIMD 指令，这个代表单指令多数据流，这个的基础意义是，如果你使用了 built-in 函数,像 np.function 或者并不要求你实现循环的函数，它可以让 python 的充分利用并行化计算。GPU更加擅长SIMD计算，但是CPU事实上也不是太差，可能没有 GPU 那么擅长吧。

### 2.10 More vectorization examples

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200405175618815.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200405183907612.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200405184152115.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200405184158595.png)

### 2.11 Vectorizing Logistic Regression

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200405191626506.png)

```python
Z = np.dot(w.T,x) + b
```

如上式，python 可以会通过广播机制将 b 拓展成 $1*m$ 的行向量

### 2.12 Vectorizing Logistic Regression's Gradient Computation

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200405154538251.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200405154829729.png)

### 2.13 Broadcasting in Python

[代码地址](https://github.com/qianlinyi/deep-learning/blob/main/Broadcasting%20example.ipynb)

### 2.14 A note on python/numpy vectors

```python
a = np.random.randn(5) -> a.shape=(5,)
a = np.random.randn(5,1) -> a.shape=(5,1)
```

上式中的第一个定义得到的是一个一维数组，既不是行向量也不是列向量，这个就会影响后续的向量运算。为解决上述问题，一种是把 a 直接定义成行向量或者列向量，或者在定义之后加断言，或者 reshape。

因此，要简化代码，不要使用一维数组。总是使用 n ∗ 1 维矩阵（基本上是列向量），或者 1 ∗ n 维矩阵（基本上是行向量），这样你可以减少很多使用 assert 语句来检查矩阵和数组维数的时间。另外，为确保矩阵或向量是所需要的维数时，不要羞于 reshape 操作。

```python
assert(a.shape == (5,1))
a = a.reshape((5,1))
```

### 2.15 Explanation of logistic regression cost function

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200415160744120.png)

可以将上述两个公式进行合并：

$\mathrm{p(y|x)=\hat{y}^y(1-\hat{y})^{(1-y)}}$

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020041516080968.png)

由于 log 函数是严格单调递增的函数，最大化 $\log(\mathrm{p(y|x)})$ 等价于最大化 $\mathrm{p(y|x)}$ 并计算其对数，则通过对数函数简化为:

$\mathrm{y\log{\hat{y}}+(1-y)\log{(1-\hat{y})}}$

这就是我们前面提到的损失函数的负数 $\mathrm{-L(\hat{y},y)}$，前面有一个负号的原因是当你训练学习算法时需要算法输出值的概率是最大的（以最大的概率预测这个值），然而在逻辑回归中我们需要最小化损失函数，因此最小化损失函数就是最大化 $\log(\mathrm{p(y|x)})$

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020041516082487.png)

  在 m 个训练样本的整个训练集中又该如何表示呢，假设所有的训练样本服从同一分布且相互独立，也即独立同分布的，所有这些样本的联合概率就是每个样本概率的乘积:

$\mathrm{P(labels\ in\ training\ set)=\prod\limits_{i=1}^{n}P(y^{(i)}|x^{(i)})}$

如果你想做最大似然估计，需要寻找一组参数，使得给定样本的观测值概率最大，但令这个概率最大化等价于令其对数最大化，在等式两边取对数：

$\mathrm{P(labels\ in\ training\ set)=\prod\limits_{i=1}^{n}P(y^{(i)}|x^{(i)})=\sum\limits_{i=1}^{m}\log{P(y^{(i)}|x^{(i)})}=\sum\limits_{i=1}^m-L(\hat{y}^{(i)},y^{(i)})}$

将负号移到求和公式外面，就得到了逻辑回归的成本函数：

$\mathrm{J(w,b)=\sum\limits_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})}$

由于训练模型时，目标是让成本函数最小化，所以我们不是直接用最大似然概率，要去掉这里的负号，最后为了方便，可以对成本函数进行适当的缩放，我们就在前面加一个额外的常数因子 $\frac{1}{m}$ ，即:


$\mathrm{J(w,b)=\frac{1}{m}\sum\limits_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})}$

## Week 3 Shallow neural networks

### 3.1 Neural Networks Overview

使用符号 $^{[m]}$ 表示和第 m 层网络相关的量

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200406093426798.png)

### 3.2 Neural Network Representation

下图是一个两层的神经网络，因为在计算网络层数时，输入层不算入总层数内

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200406094118292.png)

### 3.3 Computing a Neural Network's Output

神经网络的计算

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200416174641678.png?)

逻辑回归的计算有两个步骤，首先按步骤计算出 z，接着以 sigmoid 函数为激活函数计算 z，一个神经网络会重复很多这样的计算。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200406095048894.png)

### 3.4 Vectorizing across multiple examples

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020040610004118.png)



在垂直方向，这个垂直索引对应于神经网络中的不同节点。例如，节点位于矩阵的最左上角对应于激活单元，它是位于第一个训练样本上的第一个隐藏单元。它的下一个值对应于第二个隐藏单元的激活值。它是位于第一个训练样本上的，以及第一个训练样本中第三个隐藏单元等等。

当水平扫描，将从第一个训练示例中从第一个隐藏的单元到第二个训练样本，第三个训练样本……直到节点对应于第一个隐藏单元的激活值，且这个隐藏单元是位于这 m 个训练样本中的最终训练样本。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200406100935188.png)

### 3.5 Justification for vectorized implementation

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200406101857608.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200406102105185.png)

### 3.6 Activation functions

tanh 函数（双曲正切函数），为 sigmoid 向下平移和伸缩后的结果

$\mathrm{tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}}$

结果表明，在隐藏层上使用 tanh 函数的效果总是优于 sigmoid 函数，因为函数值域为 [-1,1]，其均值更接近于 0。但有一个例外，在二分类问题中，因为 y 的值是 0 或 1，想让 $\mathrm{\hat{y}}$ 的值介于 0 和 1之间，而不是在 -1 和 1之间，所以使用 sigmoid 函数。

sigmoid 函数和 tanh 函数两者共同的缺点是，在 z 特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于 0，从而影响梯度下降的速度。


如果输出是 0、1值（二分类问题），则输出层选择 **sigmoid** 函数，然后其它的所有单元都选择 **ReLU** 函数。

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020040610300625.png?)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200406103136587.png)

### 3.7 Why need a non-linear activation functions?

事实证明，如果你使用线性激活函数或者没有使用一个激活函数，那么无论你的神经网络有多少层一直在做的只是计算线性函数，所以不如直接去掉全部隐藏层。

在这里线性隐层一点用也没有，因为两个线性函数的组合本身就是线性函数，所以除非你引入非线性，否则无法计算更有趣的函数，即使网络层数再多也不行。

有一个例外，就是机器学习的回归问题，可以在输出层使用线性激活函数。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200406110608189.png)

### 3.8 Derivates of activation functions

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020040611111112.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200406111400824.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200406111622470.png)

### 3.9 Gradient descent for neural networks

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200406112459175.png)

axis = 1 表示水平相加求和，keepdims = True 表示保持矩阵的二维特性，防止输出秩为 1 的矩阵（列表）

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200406113142547.png)

### 3.10 Backpropagation intuition

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020040611393188.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200406114559408.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200406114737151.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200406114930757.png)

### 3.11 Random Initialization

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200406115539411.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200406115955724.png)

## Week 4 Deep Neural Networks

### 4.1 Deep L-layer neural network

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200407103736540.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200407104005590.png)

### 4.2 Forward propagation in a deep network

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200407105846930.png)

### 4.3 Getting your matrix dimensions right 

在做深度神经网络的反向传播时，一定要确认所有的矩阵维数是前后一致的，这样可以大大提高代码通过率。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200407114638276.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200407115001724.png)

### 4.4 Why deep representations?

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200407122839400.png)

### 4.5 Building blocks of deep neural networks

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200407124109689.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200407124354308.png)

### 4.6 Forward and backward propagation

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200407104357135.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200407104651930.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200407104929246.png)

### 4.7 Parameters vs Hyperparameters

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200407124936668.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200407125259241.png)
